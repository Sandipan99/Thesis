% \begin{figure*}[!ht]
% \centering
%  \includegraphics[scale=0.42]{./Fig/result.pdf}
% \caption{{\tiny (Color online) Results of the baseline model (a) and our proposed model (b) for consecutive five years after the year of
%publication of a paper. The red line in each frame corresponds to the best linear fitting (greater than 95\% confidence bounds) obtained
%from the data points in that frame.}}\label{results}
% \end{figure*}

\subsection{Performance Evaluation}\label{result}
In this section, we analyze the performance of the baseline system and our proposed model in predicting future citation count of a given
paper at the time of publication. For the baseline system, we design a model which is similar to that proposed by Yan et
al.~\cite{Yan:2011} (except that we are using a lot more features). It is identical to our
proposed model except that it does not include the first stage of our model. Thus, for
a query
paper, it takes into account all the training samples and the set of features discussed in Section~\ref{sec:feature}, 
and applies SVR to predict the citation count of the paper. 
% Note that, here also we take care of the time dependent constraint mentioned in subsection~\ref{2-stage}
% while preparing the training samples. 
Essentially, we intend to show the significance of the preprocessing stage (first stage of our model)
in the task of future citation prediction. 


\begin{table}[!t]
 \begin{center}
 \caption{Performance of the baseline model (columns 2-4) and our proposed system at various time intervals for the test papers published
between 1996-2000 (columns 5-7) and test papers published between 2001-2005 (columns 8-10).  }\label{accuracy}
 \scalebox{0.75}{
\begin{tabular}{|c|c|c|c||c|c|c||c|c|c|}
\hline
 & \multicolumn{3}{c||}{Baseline} & \multicolumn{6}{c|}{Our model}\\\cline{5-10}
 & \multicolumn{3}{c||}{1996-2000} & \multicolumn{3}{c||}{1996-2000} & \multicolumn{3}{c|}{2001-2005}\\\cline{2-10}
             & $R^2$ & $\theta$ & $\rho$ & $R^2$ & $\theta$ & $\rho$ & $R^2$ & $\theta$ & $\rho$ \\\hline
$\Delta t$=1 & {\bf 0.55}  & {\bf 5.45}     & {\bf 0.59} & 0.87 & 2.66 & 0.86 & 0.89 & 1.95 & 0.88 \\\hline
$\Delta t$=2 & 0.54  & 6.36     & 0.57 & {\bf 0.90} & {\bf 1.46} & {\bf 0.88} & {\bf 0.91} & {\bf 1.20} & {\bf 0.90} \\\hline
$\Delta t$=3 & 0.53  & 7.67     & 0.56 & 0.83 & 3.11 & 0.85 & 0.82 & 3.22 & 0.80\\\hline
$\Delta t$=4 & 0.50  & 9.16    & 0.52 & 0.77 & 3.86 & 0.84  & 0.77 & 3.76 & 0.79 \\\hline
$\Delta t$=5 & 0.48  & 12.09    & 0.49 & 0.74 & 4.18 & 0.75 & 0.71 & 4.08 & 0.73\\\hline
\end{tabular}}
\end{center}
\end{table}


\noindent \textbf{Evaluation Metrics: }For the evaluation purpose, we use the following metrics: 
coefficient of determination ($R^2$)\footnote{$R^2$ is defined as: $R^2 = 1- \frac{\sum_{d \in D} (C(d) - C^{'}(d))^2} {\sum_{d \in
D} (C(d) - C(D))^2}$, where $D$ is the set of test documents, $C(d)$ is the actual citation count for article $d$, $C^{'}(d)$ is the
predicted citation count for article $d$ in the test set $D$, $C(D)=\frac{1}{|D|} \sum_{d \in D} C(d)$ is the mean of the actual citation
count for an article present in $D$. $R^2 \leqslant 1$, and a larger $R^2$ indicates a better performance.}
\cite{Yan:2011}, mean squared error ($\theta$) and Pearson correlation coefficient ($\rho$). Note that the more the value of $R^2$ and
$\rho$, the better the accuracy of the model; but for $\theta$, the reverse
argument is true.


%We evaluate our framework in two steps corresponding to the two stages of the model: first, we evaluate the accuracy of
%the SVM classification
%module, and then we analyze the performance of the regression module. Finally, we measure the importance of
%each feature for predicting future citation count.


\noindent{\bf Dataset:}
The filtered dataset contains 1,549,317 scientific articles which need to be divided further for training and testing. However, for the evaluation of SVM, we need those papers whose true categorizations are known to us, i.e., those papers which have at least 10 years history (published between 1970-2000); though for measuring SVR accuracy, this might not be the criteria. Therefore for the sake of uniformity, we consider the papers published 
between 1970-1995 for training (505,149 papers), and the papers published between 1996-2000 (146,620 papers) for testing 
(for baseline as well as our algorithm)  throughout the evaluation (unless explicitly mentioned). However, we also report the final prediction accuracy for the papers published between 2001-2005.

 %\todo{TANMOY:I still prefer to move the results of of 2001-2005 to Appendix} 

%Unless otherwise stated, all the papers published between 1970-2000
%are considered for training (551,769 papers) and the papers published between 2001-2005 are considered for testing (392,435 papers). \\
% Note
% that, since we wish to predict the future citation count of a paper for each of the consecutive five years after publication, we do not
% consider the papers published after 2005 (605,113 papers) because they do not have at least 5 years of future citations which is required
% to validate our model. However, the citations coming from these papers to the test set are considered for the purpose of
% our prediction.\\
%PG: But citations from these papers will never come to the training
%set since you will consider the citations till 2005 only, isn't it? If so, you can remove 'training' in the above sentence.



%\subsection{Evaluation metrics}\label{metric}
% For evaluating the final prediction result of the model, we use the following standard metrics:\\
%\textbf{ Coefficient of determination ($R^2$)} \cite{ColinCameron1997, Yan:2012} is formally defined as: 
%% It is the proportion of variability in a data set that is accounted for by the statistical
%% model, which provides a measure of how well future outcomes are likely to be predicted by the model~\cite{ColinCameron1997, Yan:2012}. Formally, it is defined as:
%\begin{equation}
%R^2 = 1- \frac{\sum_{d \in D} (C(d) - C^{'}(d))^2} {\sum_{d \in D} (C(d) - C(D))^2}
%\end{equation}
%where $D$ is the set of test documents, $C(d)$ is the actual citation count for article $d$, $C^{'}(d)$ is the predicted citation
%count for article $d$ in the test set $D$, $C(D)=\frac{1}{|D|} \sum_{d \in D} C(d)$ is the mean of the actual citation counts for an
%article present in $D$. $R^2 \leqslant 1$, and a larger $R^2$ indicates a better performance.\\
%\textbf{ Mean squared error ($\theta$)} \cite{lehmann1998} is formally defined as:
%% It measures the average of the squares of the ``errors'' where the error is the amount by which the
%% value returned by the estimator differs from the quantity to be estimated~\cite{lehmann1998}. Formally, it is defined as:
%\begin{equation}
% \theta = \frac{1}{|D|} \sum_{i=1}^{n} {(C^{'}(d)-C(d))}^{2}
%\end{equation}
%where the notations refer to similar quantities mentioned earlier. The less the value of $\theta$, the more the
%predicted results are closer to the actual values.\\
%\todo{I think Pearson is not needed - Remove}
%\textbf{ Pearson correlation ($\rho$)} \cite{rodgers88} is formally defined as:
%% Pearson correlation coefficient \cite{rodgers88} between two variables is defined as the covariance of
%% the two variables divided by the product of their standard deviations. Formally given two populations $X$ and $Y$, it is defined as:
%\begin{equation}
% \rho=\frac{COV(X,Y)}{\sigma_X \cdot \sigma_Y}=\frac{E[(X-\mu_X)(Y-\mu_Y)]}{\sigma_X \cdot \sigma_Y}
%\end{equation}
%where $X$ and $Y$ are the two population, $COV(.)$ is the covariance, $\sigma_X$ is the standard deviation of $X$, $\mu_X$ is the mean of $X$, and $E[.]$ is the expectation. The more the value of $\rho$, the more the accuracy of the model.
%% Note that, the more the value
%% of
%% $R^2$ and $\rho$, the more the accuracy of the model; but for $\theta$, the reverse argument is true.
%%PG: Here also, we can get some space.


%\begin{table}
% \begin{center}
% \scalebox{0.8}{
%\begin{tabular}{|c|c|c|c||c|c|c|}
%\hline
% & \multicolumn{3}{c||}{Baseline} & \multicolumn{3}{c|}{Our model}\\\cline{2-7}
%             & $R^2$ & $\theta$ & $\rho$ & $R^2$ & $\theta$ & $\rho$ \\\hline
%$\Delta t$=1 & 0.57  & 5.56     & 0.61 & 0.89 & 1.95 & 0.88 \\\hline
%$\Delta t$=2 & 0.55  & 7.10     & 0.59 & 0.91 & 1.20 & 0.90 \\\hline
%$\Delta t$=3 & 0.52  & 8.78     & 0.55 & 0.82 & 3.22 & 0.80\\\hline
%$\Delta t$=4 & 0.50  & 10.06    & 0.45 & 0.77 & 3.76 & 0.79 \\\hline
%$\Delta t$=5 & 0.45  & 13.06    & 0.42 & 0.71 & 4.08 & 0.73\\\hline
%\end{tabular}}
%\end{center}
%\vspace{-5mm}
%\caption{Performance of the baseline model and our proposed system at various time intervals. All the
%fractional values obtained from the regression model are suitably converted into the nearest integer values since the citation count of a
%paper can not be a fractional value.}\label{accuracy}
%\end{table}







{\bf Performance of the baseline model:} The predictive performances of the baseline system for each of the consecutive five years after
publication are shown in
Table \ref{accuracy} (columns 2-4). We observe that the baseline system achieves the highest accuracy ($R^2$=0.55, $\theta$=5.45 and
$\rho$=0.59) at the immediate next year after publication of a paper. We also observe that the accuracy of the predicted citation count is moderately
overestimated for longer number of years which in turn decreases the accuracy of the baseline system in the later time periods. 





{\bf Performance of our model:}
Table \ref{accuracy} shows the final performance of our model in each time interval after the time of publication. In this table, apart from the citation prediction for the papers published between 1996-2000, we also show the accuracy for the papers published between 2001-2005 (in that case, the training set consists of papers published between
 1970-2000, and papers published between 2001-2005 constitute the test samples). Contrary to
the performance of the baseline model where the highest accuracy is achieved at the immediate next year after publication, we achieve the
best performance of our model 2 years after the year of
publication. Remarkably, we observe
that for all the cases, our model achieves
nearly 50\% higher accuracy compared to the baseline system (especially for $\theta$ and $R^2$). 
Note that the performance in 2001-2005 is also quite significant - even better than the previous regime as the system is getting trained
with more data. We further observe that the typical situation where the system performs poorly is when a new venue gets introduced and
quickly becomes popular; it takes certain number of years of learning for the system 
to predict accurately.  
Another important observation is that the predicted
citation counts are almost always overestimated (not underestimated) for the later years. 
The reason behind this is not exactly clear but the result itself provides a future opportunity to estimate a linear offset to 
predict more accurately. However, this issue is out of the scope of this paper. 
\if{0}
The reason could be that the actual citation counts of the training samples in each category are not uniform although their patterns
are
similar; few of these are very highly-cited papers, while others are not. Therefore, we believe that even if we use proper normalization at 
the time of classification, another level of normalization might also be required at the time of actual prediction. This analysis requires
more investigations, and thus remains an open area for future investigations.



\begin{figure}[!h]
\centering
 \includegraphics[scale=0.32]{./texfiles/Chapter_4/Fig/bucket.pdf}
\caption{(a) Distribution of 5-years cumulative citation count ($c_5$); values of (b) $R^2$, (c) $\theta$ and (d) $\rho$ in four different
buckets of $c_5$ for both baseline and our models. The range of $c_5$ in each bin is as follows: 1:0-2; 2:3-5; 3:6-10; 4:11-3045. Note that
the more the value of $R^2$ and $\rho$, the more the accuracy of the model; but for $\theta$, the reverse argument is true.}\label{bucket}
\end{figure}

\fi
% \begin{figure*}[ht!]
% \centering
%  \includegraphics[scale=0.42]{./texfiles/Chapter_4/Fig/example.pdf}
% \caption{(Color online) Results of the baseline and our proposed models for five representative test articles. For each article,
% the cumulative citation count within the first five years after publication is reported in parenthesis in the title of each frame.
% }\label{sample}
% \end{figure*}



\if{0}
\subsection{Performance Evaluation Considering Different Citation Ranges}
In order to compare the performances of these two models in different ranges of citation, we further look into the results obtained from the test set. First, we plot the distribution of cumulative 5-years
citations (denoted by $c_5$) for test samples in Figure~\ref{bucket}(a). Then we divide the entire range of $c_5$ into four bins such that all bins contain nearly equal number of papers, and for each
individual bin we measure the values of the three evaluation metrics. Note that here for each bin we measure the average value of each
evaluation metric over five different values of $\Delta t$. It is apparent from Figures \ref{bucket}(b)-\ref{bucket}(d) that the
performance of our system increases with the increase of $c_5$; whereas the performance of the baseline model seems to be the best in the
middle range of $c_5$. Therefore, we believe that our model might
serve as an useful tool in early prediction of the important papers that are
going to be popular in the near future. We take some example papers and individually compare their original and predicted citation counts.



\noindent \textbf{Further analysis of papers in different citation ranges:}
Figure~\ref{sample} shows the outputs of the baseline system and our proposed model for five representative scientific articles (with
their cumulative citation counts within first 5 years after publication). Note that the representative articles are chosen to
represent various ranges of cumulative citation counts. The idea is to illustrate that our technique outperforms the baseline for high
($\geq$ 300) as well as medium ($<$300 and $\geq$ 100) and low ($<$ 100) citation count ranges. Further, in each range the articles for
which the outcomes of our model are significantly different from the baseline have been a more preferable choice. The representative
articles are
as follows:
% \begin{itemize}
%  \item \textbf{ Pang et al.}, Thumbs up? Sentiment Classification using Machine Learning Techniques, EMNLP, pp. 79-86, 2002.  
%  \item \textbf{ Getoor et al.}, Learning Probabilistic Models of Link Structure, JMLR, 3, pp. 679-707, 2002.
%  \item \textbf{ J. E. Hirsch}, An index to quantify an individual's scientific research output,  PNAS , 102:46, pp. 16569-16572, 2005.
%  \item \textbf{ Carmona et al.}, A structural encoding technique for the synthesis of asynchronous circuits, FUIN , 50:2, pp. 135-154, 2002.
% \item \textbf{ D. J. Lehmann}, Non-monotonic Logics and Semantics, LOGCOM , 11:2, pp. 229-256, 2001
% \end{itemize}
\begin{itemize}
 \item \textbf{ McCanne et al.}, Receiver-driven layered multicast, CCR, 24:4, pp. 117-130, 1996. 
 \item \textbf{ Jain et al.}, Statistical pattern recognition: A review, PAMI, 22:1, pp. 4-37, 2000.
 \item \textbf{ T. Kohonen}, The self-organizing map, IJON, 21:3, pp. 1-6, 1998.
 \item \textbf{ Greenhalgh et al.}, A QoS architecture for collaborative virtual environments, ACM - MM, pp. 121-130, 1999.
\item \textbf{ Srinivasan et al.}, An assessment of submissions made to the predictive toxicology evaluation challenge, IJCAI, pp. 270-275, 1999.
\end{itemize}

One can clearly notice an almost perfect alignment of the future citation count predicted by our model with the actual citation count in comparison
to that for the baseline system for different values of $\Delta t$. Moreover, we observe that in many cases the baseline system
even fails to estimate the basic citation pattern which is yet to manifest for a particular paper, hence making 
costly mistakes (see Figures \ref{sample}(a) - (c)). 

\fi

\begin{table}[!t]
\centering
\caption{Confusion matrix depicting the performance of SVM at the first stage of our prediction model. The
last column
indicates the accuracy of the classification system for each individual category. The correct classification results (diagonal
elements) are highlighted in bold font.}\label{confusion}
\scalebox{0.62}{
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
    &  PeakInit & PeakMul & PeakLate & MonDec & MonIncr & Oth & Accuracy\\\hline\hline
PeakInit & \textbf{ 9550} & 70 & 20 & 20 & 0 & 2419 & 0.79\\\hline 
PeakMul & 29 & \textbf{ 15261} & 2500 &  3 & 0 &  3000  & 0.73\\\hline 
PeakLate & 7 &  718 &  \textbf{ 4842} &  2 &  489 & 518 & 0.73\\\hline 
MonDec & 398 &  444 &  157 &  \textbf{ 2247} & 0 &  453 & 0.61\\\hline  
MonIncr & 2 &  403 &  0 &  0 &  \textbf{ 2789} & 0 & 0.87\\\hline
Oth& 55 &  5142 &  5 &  2 &  0 &  \textbf{ 154188} & 0.96\\\hline\hline
\multicolumn{7}{|c|}{Overall accuracy} & 0.78 \\\hline
\end{tabular}}
\end{table}

\textbf{Performance of SVM classification:}
We have discussed the accuracy of the prediction model but this in turn depends on the underlying first stage of classification which is done using multi-class SVM. 
Table \ref{confusion} shows the confusion matrix describing the performance of the SVM classification used in the first stage of our
model. Each column of the matrix represents
the instances
in a predicted class, while each row represents the instances in an actual class. Therefore, all correct guesses are located in the diagonal
of the table. Bethard and Jurafsky \cite{bethard2010} mentioned that 90\% of papers that have been published in academic journals are never
cited. We have also observed in Figure \ref{profile} that our dataset is highly biased towards the population of the low-cited papers
(i.e,
`Oth'). Therefore, SVM also slightly overestimates `Oth' category in the classification. The overall accuracy of the classification
system is 0.78 which is quite promising considering the biased training samples and the fact that no feature after the publication of the paper is considered to classify the papers.
Besides `Oth' category, we also observe higher accuracy for class `MonIncr' (0.87) which is followed by `PeakInit' (0.79),
`PeakMul' (0.73) and `PeakLate' (0.73). The lowest accuracy is
obtained for category `MonDec' (0.61). One possible reason could be that
this is one of the rarest categories in the dataset. Thus, the lack of enough
evidences might have accounted for the low final accuracy of the SVM
model in classifying the papers into this category.                                                                                            



\textbf{Performance assuming perfectly accurate SVM:}
In Table \ref{confusion}, we notice that in the first stage of our model, we achieve overall 78\% accuracy and the error in this stage
propagates in the second stage of our model. We believe that this performance can be improved a lot in future with more efficient feature
selection and thus remains a potential area of future research. However, one might argue that if the SVM model could have achieved nearly
100\%
accuracy, how much improvement one would expect from the final prediction model. This might also answer how the error which propagates
from
the first stage of the model to the next stage affects the final output of citation prediction.
Since we know the true category of each of the test papers, we use only those training samples belonging to the true category for training SVR, thus forcing 100\% accuracy in the first stage. Table~\ref{comparison} shows the performance improvements (differences) of our model in comparison to the earlier results shown in Table \ref{accuracy} for different values of $\Delta t$ (test set constitutes papers published within 1996-2000). One can clearly notice a significant improvement over the baseline model and our earlier results especially for the higher values of $\Delta t$. This indicates that the error propagating from the first stage SVM model to
the next stage significantly affects long term citation prediction, and improvements in the first stage can highly enhance the overall performance of the system. 

\begin{table}[!t]
 \begin{center}
 \caption{Performance improvement (differences) of our model in comparison to the earlier results shown in Table \ref{accuracy} for
different values of $\Delta t$, while considering 100\% accuracy in SVM model.  }\label{comparison}
\scalebox{0.8}{
\begin{tabular}{|c|c|c|c||c|c|c|}
\hline
 & \multicolumn{3}{c||}{Improvement over} & \multicolumn{3}{c|}{Improvement over}\\
 & \multicolumn{3}{c||}{baseline model} & \multicolumn{3}{c|}{our earlier results}\\\cline{2-7}
             & $R^2$ & $\theta$ & $\rho$ & $R^2$ & $\theta$ & $\rho$ \\\hline
$\Delta t=1$ &    0.34 &  -3.54&0.31&0.02&-0.75&0.04    \\\hline
$\Delta t=2$&0.37&-5.09&0.34&0.01&-0.19&0.03    \\\hline
$\Delta t=3$&0.37&-5.85&0.33&0.07&-1.26&0.04    \\\hline
$\Delta t=4$&0.35&-7.22&0.36&0.08&-1.92&0.04    \\\hline
$\Delta t=5$&{\bf 0.41}&{\bf-10.19}&{\bf0.37}&{\bf0.15}&{\bf-2.28}&{\bf0.11}   \\\hline

\end{tabular}}
\end{center}
\end{table}


\if{0}
 
\begin{table*}[!t]
\begin{center}
\caption{ Feature analysis in two different stages of our prediction model. SVM classification: the third
column indicates the
decrease in overall accuracy when dropping each feature in isolation in comparison to the case when all the features are present
(i.e., 0.78).
SVR model: each subsequent column from the columns 4-8 (columns 9-13) indicates the Spearman's rank
correlation of each feature with the actual citation count without categorization - Baseline Model (with categorization - 
Our Model). }\label{feature_analysis}
\scalebox{0.65}{
\begin{tabular}{|c c|c||c||c|c|c|c|c||c|c|c|c|c|}
\hline
\multicolumn{3}{|c||}{} & Decrease in   & \multicolumn{5}{c||}{Correlation with the actual citation} & \multicolumn{5}{c|}{Correlation with
the actual citation}\\
\multicolumn{3}{|c||}{Features }& performance & \multicolumn{5}{c||}{count - Baseline Model} & \multicolumn{5}{c|}{count - Our
Model}\\\cline{5-14}
\multicolumn{3}{|c||}{ } & of SVM & $\Delta t$=1 & $\Delta t$=2 &$\Delta t$=3 &$\Delta t$=4 &$\Delta t$=5& $\Delta t$=1 & $\Delta t$=2
&$\Delta t$=3 &$\Delta t$=4 &$\Delta t$=5\\\hline
\multirow{8}{*}{\begin{sideways}Author-\end{sideways}} & \multirow{8}{*}{\begin{sideways}centric\end{sideways}} & AvgProAuth
& \textbf{ 0.21}&\textbf{ 0.67} & \textbf{ 0.65} & \textbf{ 0.63} & \textbf{ 0.61} & \textbf{ 0.55} & \textbf{ 0.76} & \textbf{ 0.74} &
\textbf{ 0.70} & \textbf{ 0.67} & \textbf{ 0.68}
\\\cline{3-14}	
 & &MaxProAuth & 0.05 & 0.19 & 0.20 & 0.20 & 0.20 & 0.19 & 0.28 & 0.38 & 0.30 & 0.33 & 0.21 \\\cline{3-14} 	
 & &AvgHindex & 0.06 &0.17 & 0.27 & 0.18 & 0.16 & 0.15 & 0.32 & 0.41 & 0.36 & 0.38 & 0.32\\\cline{3-14}	
 & &MaxHindex & 0.04 &0.11 & 0.13 & 0.12 & 0.10 & 0.11 & 0.34 & 0.42 & 0.39 & 0.32 & 0.29\\\cline{3-14}	
& &AvgAuthDiv &0.12 & 0.56 & 0.55 & 0.46 & 0.43 & 0.43 & 0.71 & 0.70 & 0.69&0.61 & 0.56 \\\cline{3-14}
 & &MaxAuthDiv & 0.18 &0.62 & 0.55 & 0.49 & 0.26 & 0.14 & 0.63 & 0.62 & 0.56 & 0.38 & 0.34 \\\cline{3-14}
 & &AvgNOCA & 0.10&0.33 & 0.21 & 0.17 & 0.19 & 0.18 & 0.43 & 0.52 & 0.42 & 0.38 & 0.21\\\cline{3-14}	
 & &MaxNOCA & 0.16&0.43 & 0.32 & 0.31 & 0.30 & 0.23 & 0.51 & 0.45 & 0.42 & 0.39 & 0.32\\\hline\hline
\multirow{3}{*}{\begin{sideways}Venue-\end{sideways}} & \multirow{3}{*}{\begin{sideways}centric\end{sideways}} & VenPresL &
0.12&0.47 & \textbf{ 0.42} & \textbf{ 0.23} & \textbf{ 0.14} & 0.01 & 0.57 & \textbf{ 0.58} & \textbf{ 0.51} & \textbf{ 0.49} & \textbf{
0.36}  \\\cline{3-14}
 & &VenPresS & 0.13&0.48 & 0.22 & 0.13 & 0.04 & 0.02 & 0.49 & 0.44 & 0.38 & 0.32 & 0.29\\\cline{3-14}
 & &VenDiv & \textbf{ 0.18}&\textbf{ 0.50} & 0.37 & 0.20 & 0.13 & \textbf{ 0.03} & \textbf{ 0.59} & 0.48 & 0.43 & 0.36 &0.32 \\\hline\hline
\multirow{5}{*}{\begin{sideways}Paper-\end{sideways}} & \multirow{5}{*}{\begin{sideways}centric\end{sideways}} & Team & \textbf{
0.11}&\textbf{ 0.47}   & 0.36 & 0.22 & 0.18 & 0.11 & 0.37 & \textbf{ 0.49}& \textbf{ 0.46} &0.41 & 0.37 \\\cline{3-14}
 & & RefCount & 0.01&0.18 & 0.20 & 0.21 & 0.29 & 0.31 & 0.38 & 0.43 & 0.41 & 0.37 & 0.32\\\cline{3-14}	
 & &RDI & 0.07 &0.36 & \textbf{ 0.37} & \textbf{ 0.36} & 0.35 & 0.33 & \textbf{ 0.40} & 0.46 & 0.42 & 0.39 & 0.31\\\cline{3-14}
 & &KDI & 0.03&0.17 & 0.19 & 0.13 & 0.12 & 0.12 & 0.27 & 0.36 & 0.29 & 0.22 & 0.21\\\cline{3-14}
 & &Topic & 0.10&0.23 & 0.21 & 0.25 & \textbf{ 0.38} & \textbf{ 0.40} & 0.31 & 0.35 & 0.37 & \textbf{ 0.41} & \textbf{ 0.45}\\\hline 	
\end{tabular}}
\end{center}
\end{table*}


\subsubsection{Feature analysis}\label{feature}
Here we systematically analyze the impact of different feature groups described in Section~\ref{sec:feature} for SVM classification
system as well as the actual citation count prediction. For SVM classification, we drop each feature in isolation and measure the overall
performance of the model. The third column of Table \ref{feature_analysis} (column 3) shows the decrease in overall
performance of SVM after dropping each feature in
comparison to the case where all the features are present. Author-centric features seem to be the most effective features in the
classification model. Among them, the absence of the average productivity of the authors in a paper (AvgProAuth) leads to maximum
decrease in performance which is
followed by the maximum diversity of the authors in a paper.

For measuring the impact of each feature in future citation count prediction, we use the standard approach adopted by McNamara et
al.~\cite{Daniel} --  Spearman's rank correlation coefficient, an established measure of the dependence of two variables using a monotonic
function, is taken for each of the features and the target variable (actual citation count) for each $\Delta t$. 

\noindent \textbf{Feature analysis in baseline model: }
Table~\ref{feature_analysis} (columns 4-8) shows the impact of each individual features in the baseline model. 
We observe that once again the average
productivity of authors in a paper turns out to be the best feature for all the time intervals. It is quite understandable since authors are
likely to cite papers written by reputed and influential
authors. Venue impact is also significant for the first few years. Papers from
prestigious venues are likely to be highly cited. Interestingly, most of the paper-centric features which seem to have least
significance in the initial few years, appear to be quite effective in the later time periods. This essentially indicates that a
good quality paper eventually gets appreciations from the researchers irrespective of the reputation of the authors and the publication
venue. However, it might take some time to get noticed by the others. 
\if{0}
Unexpectedly, while author productivity proves to be the most
efficient feature, the h-index of an author seems to be the most inefficient
one in predicting future citation. It indicates that the average number of papers written by an author might be the crucial
factor for
accumulating citations, rather than taking into account both the quality and quantity together (captured by h-index).
\todo{I have problems in understanding this section -- Lets discuss}
\fi

\noindent \textbf{ Observations about strong and weak features:} In Section~\ref{svr}, we have noticed that our model seems to perform well at $\Delta t$=2. We hypothesize that this might happen because
of the early categorization in the training phase. In Table \ref{feature_analysis} (columns 4-8), we notice that though the most prominent
features such as AvgProAuth,
AvgAuthDiv, Team show higher correlation with the actual citation count at $\Delta t$=1, other weak features such as
MaxProAuth, AvgHindex, VenPres, RDI and KDI tend to attain maximum correlation at $\Delta t$=2. We believe that due to
the categorization, these weak features tend to become prominent in the final prediction, resulting in highest accuracy at $\Delta
t$=2. To strengthen this hypothesis, we again measure Spearman's rank correlation for all the features in our model. 

\noindent \textbf{ Feature analysis in our model: }Table \ref{feature_analysis} (columns 9-13) reports the rank correlation of each feature with the actual citation count averaged
over all the categories. Besides the improvement of the absolute value of the correlation, one can also notice a large overall improvement of
the correlation for most of the features at $\Delta t$=2 which indeed strengthens our hypothesis. Interestingly, although the
relative ordering of the features in terms of the average correlation for all values of $\Delta t$ remains almost same without (baseline model)  and with 
categorization (our model), the weak features tend to rise significantly after categorization with reasonably higher improvement in rank correlation
and serve an important role in the final citation count prediction.

\fi


\begin{table}[!t]
\centering
\caption{(Left) Performance of the two-stage prediction model for two different types of categorization schemes; (Right) performance of the
baseline model and our proposed system at various time intervals after including the first year's citation
count as another feature. }\label{accuracy1}
\scalebox{0.8}{
\begin{tabular}{|c|c|c|c||c|c|c|}
\hline
 \multicolumn{7}{|c|}{Performance of two-stage prediction model}\\\hline
 & \multicolumn{3}{c||}{Cat-1} & \multicolumn{3}{c|}{Cat-2}\\\cline{2-7}
             & $R^2$ & $\theta$ & $\rho$ & $R^2$ & $\theta$ & $\rho$ \\\hline
$\Delta t$=1 & 0.87  & 2.05     & 0.85 & 0.59 & 5.23 & 0.63 \\\hline
$\Delta t$=2 & {\bf0.88}  & {\bf1.94}     & {\bf0.88} & {\bf0.61} & {\bf4.67} & {\bf0.68} \\\hline
$\Delta t$=3 & 0.79  & 3.38     & 0.80 & 0.55 & 6.86 & 0.61\\\hline
$\Delta t$=4 & 0.75  & 4.01    & 0.76 & 0.51&  8.89 & 0.54 \\\hline
$\Delta t$=5 & 0.71  & 4.10    & 0.72 & 0.50 & 9.58 & 0.49\\\hline
\end{tabular}}
\quad
 \scalebox{0.9}{
\begin{tabular}{|c|c|c|c||c|c|c|}
\hline
 & \multicolumn{3}{c||}{Baseline} & \multicolumn{3}{c|}{Our model}\\\cline{2-7}
             & $R^2$ & $\theta$ & $\rho$ & $R^2$ & $\theta$ & $\rho$ \\\hline
$\Delta t$=2 & {\bf0.60}  & {\bf4.92}     & {\bf0.65} & {\bf0.92} & {\bf1.02} & {\bf0.90} \\\hline
$\Delta t$=3 & 0.59  & 5.06     & 0.64 & 0.85 & 2.56 & 0.82\\\hline
$\Delta t$=4 & 0.58  & 5.44     & 0.62 & 0.83 & 3.16 & 0.81 \\\hline
$\Delta t$=5 & 0.54  & 6.56     & 0.56 & 0.81 & 3.88 & 0.79\\\hline
\end{tabular}}
\end{table}


\textbf{Robustness of categories:} Earlier results show that the systematic
categorization of the training samples improves the performance of the prediction system in comparison to the baseline system. A pertinent
question could be that how robust are these categories for the final prediction, i.e., if the
(near-)similar/dissimilar categories are merged together, how does it affect the final output of the model. Note that in
Figure~\ref{profile}, the categories `PeakInit' and `MonDec' (`PeakLate' and `MonIncr') are nearly similar in terms of the
number of peaks and whether the peak occurs in the first/last half of the citation profile; others are reasonably different. Now the
question is that if we merge the near-similar categories together to reduce the total number of categories, how does it affect the final
prediction. The extreme case would be the baseline system itself where all the categories are combined. Apart from this, we reconfigure the
categorization in two different ways: \textbf{ [Cat-1]} combining near-similar categories and keeping others separate ([PeakInit + MonDec],
[PeakLate + MonIncr], [PeakMul], [Oth]), \textbf{ [Cat-2]} combining one pair of dissimilar categories ([PeakInit + PeakMul], [PeakLate],
[MonDec], [MonIncr], [Oth]). In this case also, we use the default set of training and test samples as mentioned in earlier in the dataset
and run the two-stage prediction model separately for two types of categorization.

Table~\ref{accuracy1} (left) shows the final performance of the two-stage model for the two categorization schemes. One can easily notice
two
immediate consequences of these schemes -- (i) combining two near-similar categories (as done in Cat-1) does not make much effect on the
final
prediction in comparison to combining two different categories (as followed in Cat-2), since the decrease in accuracy from the actual
results (shown in Table~\ref{accuracy}) is significantly less for Cat-1 than that for Cat-2;
%PG: The above line is not clear=> done
(ii) while combining two major categories in Cat-2, the accuracy of the final prediction decreases drastically from the actual results of Table~\ref{accuracy}, and it tends to be closer to the baseline system. 
The results for Cat-1 are still  worse (although slightly)  than the original six category system.
Hence, a natural question stays whether dividing the data into further categories would improve performance. We have
tried different variations; all of them tend to introduce more noise in the SVM classification module thus decreasing the
overall performance. 
% However a more systematic category study is an important future work. 
%From these results, we can conclude that the resolution of this
%categorization might play a vital role depending on the expectted accuracy of the problem under consideration.



% \begin{table}[h!]
%  \begin{center}
%  \caption{Performance of the two-stage prediction model for two different types of categorization schemes. }\label{accuracy1}
%  \scalebox{0.8}{
% \begin{tabular}{|c|c|c|c||c|c|c|}
% \hline
%  \multicolumn{7}{|c|}{Performance of two-stage prediction model}\\\hline
%  & \multicolumn{3}{c||}{Cat-1} & \multicolumn{3}{c|}{Cat-2}\\\cline{2-7}
%              & $R^2$ & $\theta$ & $\rho$ & $R^2$ & $\theta$ & $\rho$ \\\hline
% $\Delta t$=1 & 0.87  & 2.05     & 0.85 & 0.59 & 5.23 & 0.63 \\\hline
% $\Delta t$=2 & {\bf0.88}  & {\bf1.94}     & {\bf0.88} & {\bf0.61} & {\bf4.67} & {\bf0.68} \\\hline
% $\Delta t$=3 & 0.79  & 3.38     & 0.80 & 0.55 & 6.86 & 0.61\\\hline
% $\Delta t$=4 & 0.75  & 4.01    & 0.76 & 0.51&  8.89 & 0.54 \\\hline
% $\Delta t$=5 & 0.71  & 4.10    & 0.72 & 0.50 & 9.58 & 0.49\\\hline
% \end{tabular}}
% \end{center}
% \end{table}




\textbf{Impact of early citation information:}
In earlier papers~\cite{Castillo}, it has been shown that the citation count of a paper in the initial few years after publication plays an important role in predicting the future citation count of the paper. However, in our experiments, we have only considered those features of a paper that one can get at the time of its publication since our objective is to predict the future impact of a paper as early as possible. However, we also believe that the initial few years' citation counts can boost up the prediction of the final citation counts since these initial citations seem to be the early crowd-sourced feedback of the scientific community about the paper. Therefore, to see its impact in the final prediction, we conduct another set of
experiments -- we include the citation count
of a paper in the immediate next year ($\Delta t$=1) of its publication as a feature and predict the citation count of each paper for
$\Delta t$ between 2 and 5 years.  Table~\ref{accuracy1} (right) shows the accuracy for both the baseline
system and the two-stage prediction model. As compared to Table~\ref{accuracy}, we can see a clear improvement of the system mostly in the
higher values of $\Delta t$. Moreover, this also improves the SVM classification where we achieve 84\% overall accuracy. With this
information, the baseline system also improves a lot as mentioned in \cite{Yan:2011}. 


% \begin{table}[h!]
%  \begin{center}
%  \caption{Performance of the baseline model and our proposed system at various time intervals after including the first year's citation
% count as another feature. }\label{citation_accuracy}
%  \scalebox{0.9}{
% \begin{tabular}{|c|c|c|c||c|c|c|}
% \hline
%  & \multicolumn{3}{c||}{Baseline} & \multicolumn{3}{c|}{Our model}\\\cline{2-7}
%              & $R^2$ & $\theta$ & $\rho$ & $R^2$ & $\theta$ & $\rho$ \\\hline
% $\Delta t$=2 & {\bf0.60}  & {\bf4.92}     & {\bf0.65} & {\bf0.92} & {\bf1.02} & {\bf0.90} \\\hline
% $\Delta t$=3 & 0.59  & 5.06     & 0.64 & 0.85 & 2.56 & 0.82\\\hline
% $\Delta t$=4 & 0.58  & 5.44     & 0.62 & 0.83 & 3.16 & 0.81 \\\hline
% $\Delta t$=5 & 0.54  & 6.56     & 0.56 & 0.81 & 3.88 & 0.79\\\hline
% \end{tabular}}
% \end{center}
% \end{table}


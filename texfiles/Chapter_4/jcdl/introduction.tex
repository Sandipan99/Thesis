\noindent

Peer-review system has been relied upon by the scientific community for determining the correctness and the quality of the findings presented in a research article. The authenticity and, hence, the need for this process has long been debated since in many cases flawed research has got into the literature even though the peer-review process was rigorous \cite{bohannon2013s}. Similarly, there have been cases where excellent research was misjudged by the peer-review process and therefore rejected \cite{braatz2014papers}. The publishing house makes significant investments into ensuring the quality of editing and reviewing of the received submissions and, therefore, identifying the necessity of this entire system is of prime importance. 

\noindent{\bf Debates on the scientific peer-review:} 

The effectiveness of peer-review have been studied to a large extent in the domain of medical sciences where peer-review is heavily relied upon for judging the quality of a research article ~\cite{jefferson2006editorial,kassirer1994peer,
rennie1990editorial}. The effect of blinding on the quality of peer review has also been studied in detail~\cite{jefferson2002measuring, mcnutt1990effects}. It was observed that blinding improves the quality of reviews. Several limitations of the review process have also been pointed out~\cite{horrobin1990philosophical}. In~\cite{cole1981chance} the authors show that there is a high degree of disagreement within the population of eligible reviewers.~\cite{braatz2014papers} also shows that there are a significant number of papers that receive more citations after rejection. All these together point to limitations of the review process and have resulted in the scientific community questioning the requirement of this process. 

\noindent{\bf A massive peer-review dataset:} In this paper, we investigate the effectiveness of the peer-review system through a rigorous and large-scale analysis of the scientific review data. In particular, we consider a set of around $29k$ papers along with roughly $70k$ unique review reports containing $12m$ lines of review text submitted to the Journal of High Energy Physics (JHEP) between 1997 and 2015. We would like to point out here that this dataset is unique as well as very rich and we do not know of any other work that presents such a large-scale analytics of an equivalent dataset. Informed with the details of the number of reviews per paper, the content of the review reports and the citation counts we perform, for the first time, a series of systematic measurements to determine whether the peer-review process is indeed able to correctly differentiate between high impact contributions and the rest.

\noindent{\bf Citation impact of accepted papers:}  Assuming that citation count of a paper is representative of its overall quality, we observe that on average those papers which were accepted at JHEP after passing through the peer-review process, are cited more often compared to those which got rejected at JHEP and eventually got accepted at a different venue. While this is true for the majority, there are a few exception cases where either a rejected paper is found to receive high citations or an accepted paper is found to receive (almost) no citation. 

\noindent{\bf Reviewer-reviewer interaction network:} One of the central contributions of this work is the introduction of a novel reviewer-reviewer interaction network built as the one-mode projection of the editor-reviewer bipartite network. The reviewer-reviewer interaction network has nodes as the reviewers and two reviewers are connected by an edge if they have been assigned by the same editor. Surprisingly, the network related structural features such as the degree, the clustering coefficient and the centrality values (closeness, betweenness etc.) of the reviewer nodes in the reviewer-reviewer network strongly correlate with the  long-term citations received by the papers these reviewers refereed. 

\noindent{\bf Supporting features:} Another unique contribution of this paper is that we also build a set of supporting features based on the various characteristics of the papers submitted as well as the authors and the referees of the submitted papers. {\em Papers}: The highly cited papers tend to undergo lesser rounds of review and there also exists an optimal team size (number of contributing authors) for which the accrued citation is maximum.
{\em Review Reports}: For the accepted papers, the length of the review reports seem to be indicator of the long-term citation. Moreover there exists an optimal length for which the citation obtained by the corresponding paper is maximum. On performing sentiment analysis, we observe the review reports to be mostly neutral as the referees hardly use highly polar words in their reports. However, we observe several linguistic quality indicators which can be extracted from the review text that determines whether a paper is going to be cited well in the future. {\em Authors}: From the author specific analysis we observe that for authors who have a higher acceptance to submission ratio tend to receive more citations than others who have a lower acceptance to submission ratio. In addition, the reviews received by authors having higher acceptance to submission ratio tend to contain more positive sentiments on average. {\em Reviewers}: In a previous work~\cite{sikdar2016anomalies} the authors showed that the reviewers who tend to accept or reject most of the papers assigned to them fail to correctly judge the quality of the papers. We include such history based features of the reviewers in the set of supporting features. 

Two further interesting observations from the analysis of the supporting features are -- the low cited accepted papers got in due to the higher accept history of the authors and the lenience of the referees; the high cited rejected papers could not make a place because of lower accept history of the authors and the strictness of the referees.

\noindent{\bf Determining the fate of the paper:} 
Based on the network features built above, we propose a supervised model which quite accurately predicts ($R^2$ = {\bf 0.79}, $RMSE$ = {\bf 0.496}) the long-term citation of a paper. In addition, if we also include the supporting features into the model we obtain further gains ($R^2$ = {\bf 0.81}, $RMSE$ = {\bf 0.46}). Analysis of the importance of the features shows that the network features are the strongest predictors for this task.
We believe that our system would be of immense help in assisting the editor in deciding acceptance or rejection of the paper specifically in cases when the review reports are contradictory. 
Note that while our work is a case study of JHEP, the formulations that we report are very general and can be extended to any other available dataset. 

\medskip
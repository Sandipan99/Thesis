\section{Survey on Community Detection and Evaluation}
In this section, we survey the current literature on the community identification problem and other closely related problems. First, we
review
the work on identifying non-overlapping and overlapping communities in different networks. Following this, we present various metrics used
to evaluate the community structures.

\subsection{Non-overlapping Community Detection}\label{nonover_comm}
A wide spectrum of community detection methods have been proposed to detect disjoint communities from static networks. Interested readers
are encouraged to read the following survey papers: Fortunato \cite{Fortunato201075}, Lancichinetti, Fortunato \cite{Lancichinetti},
Harenberg et al. \cite{WICS1319}. All these algorithms can
be roughly divided into the following categories.

\subsubsection*{Traditional Methods}
\noindent{\bf (i) Graph partitioning:}  The problem of graph partitioning consists of dividing the vertices in different groups of
predefined size, such that the number of edges lying between the groups is minimal. The number of edges running between clusters is called 
{\em cut size}. There are several algorithms that can do a good job, even if their solutions are not necessarily optimal
\cite{Pothen:1997,Kernighan70}. Another popular technique is the spectral bisection method \cite{opac-b1007877}, which is based
on the properties of the spectrum of the Laplacian matrix.  Graphs can be also partitioned by minimizing measures that are affine to
the cut size, like {\em conductance}
\cite{Bollobas1998}, {\em ratio cut} \cite{citeulike:10267542} and {\em normalized cut} \cite{Shi:2000}. Algorithms for graph partitioning
are not good for community detection, because it is necessary to provide as input the number of groups and in some cases even their sizes,
about which in principle has no prior information.

\noindent{\bf (ii) Hierarchical clustering:} Most of the real-world graphs have a hierarchical structure, i.e., display several
levels of grouping of the vertices, with small clusters included within large clusters, which are in turn included in larger clusters, and
so on. In such cases, one may use hierarchical clustering algorithms \cite{hastie01}, i.e. clustering techniques that reveal the
multilevel structure of the graph. Hierarchical clustering techniques can
be classified in two categories: Agglomerative (bottom-up) and Divisive (top-down) algorithms. Hierarchical clustering has the advantage
that it does not require a prior knowledge of the number and size of the clusters. However, it does not provide a way to discriminate
between many partitions obtained by the procedure, and to choose that or those partitions which better represent the community structure of
the
graph. The results of the method depend on the specific similarity measure adopted. The procedure also yields a hierarchical structure by
construction, which is rather artificial in most cases, since the graph at hand may not have a hierarchical structure at all
\cite{citeulike:646478}. 

\noindent{\bf (iii) Partitional clustering:} Partitional clustering assumes that the number of clusters is predefined, say $k$. The points 
are embedded in a metric space, so that each vertex is a point and a distance measure is defined between pairs of points in the space. The
distance is a measure of dissimilarity between vertices. The goal is to separate the points in $k$ clusters so as to maximize/minimize a
cost
function based on distances between points and/or from points to {\em centroids}. Few such functions include minimum $k$-clustering,
$k$-clustering sum, $k$-center, $k$-median. The most popular partitional technique in the literature is $k$-means clustering \cite{kmeans}.
Extensions of k-means clustering to graphs have been proposed by some authors \cite{HlaouiW06,Bezdek:1981}. The limitation of partitional
clustering is
the same as that of the graph partitioning algorithms: the number of clusters must be specified at the beginning, the method is not able to
derive it. 

\noindent{\bf (iv) Spectral clustering:} Spectral clustering includes all methods and techniques that partition the set of vertices into
clusters by using the eigenvectors of matrices or other matrices derived from it. In particular, the objects could be points in some metric
space, or the vertices of a graph. Spectral clustering consists of a transformation of the initial set of objects into a set of points in
space, whose coordinates are elements of eigenvectors. The set of points is then clustered via standard techniques, like $k$-means
clustering. The first contribution on spectral clustering was by Donath and Hoffmann \cite{Donath:1973}.  There are three
popular methods of
spectral clustering: unnormalized spectral clustering and two normalized spectral clustering techniques, proposed by Shi and Malik
\cite{Shi:2000} and by Ng et al. \cite{Ng01on} respectively. However, Nadler and Galun \cite{nadler} discussed the limitations of this
method such as it
cannot successfully cluster datasets that contain structures at different scales of size and density. 
 
\subsubsection*{Divisive Algorithms}
The philosophy of divisive algorithms is to detect the edges that connect vertices of different communities and remove them, so that the
clusters get disconnected from each other. The most popular algorithm is the one proposed by Girvan and Newman \cite{ng2002}. The
method is
historically important, because it marked the beginning of a new era in the field of community detection. Here edges are selected according
to the values of measures of {\em edge betweenness centrality}. Tyler et al. proposed a modification of the Girvan-Newman algorithm, to
improve the speed of the calculation \cite{gcalda:TWH03}. Another fast version of the Girvan-Newman algorithm has been proposed by Rattigan
et al. \cite{Rattigan:2007}. Here,
a quick approximation of the edge betweenness values is carried out by using a network structure index, which consists of a set of vertex
annotations combined with a distance measure. 
% Holme et al. \cite{HolmeHJ03} have used a modified version of the algorithm in which vertices,
% rather than edges, are removed. 
In this line, gradually two community detection algorithms have been proposed for overlapping community
detection, namely the concept of vertex splitting \cite{PinWes06} and CONGA (Cluster Overlap Newman-Girvan Algorithm) \cite{Gregory:2007}.

\if{0}
Since communities are assumed to have high density of edges, it is reasonable to expect cycles, i.e, closed non-intersecting paths; while
edges connecting two communities will hardly be part of cycles. Based on this idea, Radicchi et al. \cite{radicchi} proposed a new
measure, {\em edge clustering coefficient} (a generalization of clustering coefficient introduced by Watts and Strogatz \cite{Watts-1998}
for
vertices), such that low values of the measure are likely to correspond to inter-community edges. In a successive paper, the authors
extended the method to the case of weighted networks \cite{Castellano04}, which was further extended to
bipartite networks by Zhang et al. \cite{ZhaWanZha07}. An alternative measure of centrality for edges is information centrality. It is based
on the concept of efficiency \cite{PhysRevLett.87.198701}, which estimates how easily information travels on a graph according to the
length of shortest paths between vertices. In the algorithm by Fortunato et al. \cite{ForLatMar04} , edges are removed according to
decreasing values of information centrality. Since the procedure is iterated until there are no more edges in the network, the final
complexity is $O(m^3n)$, or $O(n^4)$ on a sparse graph.
\fi

\subsubsection*{Modularity-based Algorithms}
Modularity (introduced by Newman and Girvan \cite{ng2002}) is by far the most used and best known quality function. It is based on the
idea that a random graph is not expected to have a cluster structure, so the actual strength of clusters is revealed by the comparison
between the actual density of edges in a subgraph and the density one would expect to have in the subgraph if the vertices of the graph were
attached regardless of community structure. This expected edge density depends on the chosen null model, i.e., a copy of the original graph
retaining some of its structural properties but not community structure. Modularity can then be written as follows:

\begin{equation}\label{modularity}
 Q=\frac{1}{2m}\sum_{ij}(A_{ij} - \frac{k_i k_j}{2m})\delta(C_i,C_j)
\end{equation}
where the sum runs over all pairs of vertices, $A$ is the adjacency matrix, $m$ the total number of edges of the graph, $k_i$ the degree of
vertex $i$, the $\delta$-function yields one if vertices $i$ and $j$ are in the same community ($C_i = C_j$), zero otherwise. By assumption,
high values of modularity indicate good partitions. All clustering techniques that require modularity, directly and/or indirectly can be
classified as follows.

\noindent{\bf (i) Greedy techniques:} The first algorithm devised to maximize modularity was a greedy method proposed by Newman
\cite{PhysRevE.69.066133}. It is an agglomerative hierarchical clustering method, where groups of vertices are successively joined to form
larger communities such that modularity increases after the merging. Later on, Clauset et al. \cite{cnm2004} proposed more efficient data
structure like {\em max-heaps} to
make Newman's algorithm faster.  Danon et al. \cite{Danon2006} suggested to normalize the modularity variation
$\Delta Q$ produced by the merger of two communities by the fraction of edges incident to one of the two communities, in order to favor
small clusters. Wakita and Tsurumi \cite{Wakita:2007} noticed that, due to the bias towards large communities, the fast
algorithm by Clauset et al. is inefficient, because it yields very unbalanced dendrograms. Another trick to avoid the formation of large
communities was proposed by Schuetz and Caflisch \cite{SC08}. A different greedy approach has been introduced by Blondel et al.
\cite{bgll2008} (mostly known as {\em Louvain
algorithm}), for the general case of weighted graphs. The method consists of two phases. First, it looks for ``small'' communities by
optimizing modularity locally. Second, it aggregates nodes of the same community and builds a new network whose nodes are the
communities obtained in the first stage. These steps are repeated iteratively until a maximum of modularity is attained. The modularity
maxima found by the method are
better than those found with the greedy techniques by Clauset et al. \cite{cnm2004} and Wakita and Tsurumi \cite{Wakita:2007}.


\noindent{\bf (ii) Simulated annealing:} Simulated annealing \cite{Kirkpatrick1983} is a probabilistic procedure for global optimization
used in different fields and problems. It was first employed for modularity optimization by Guimera et al. \cite{guimera2004}. Its
standard implementation combines two types of moves: {\em local moves}, where a single vertex is shifted from one cluster to another, taken
at random; {\em global moves}, consisting of mergers and splits of communities. Splits can be carried out in several distinct ways. The best
performance is achieved if one optimizes the modularity of a bipartition of the cluster, taken as an isolated graph. Global moves reduce the
risk of getting trapped in local minima and they have proven to lead to much better optima than using simply local moves
\cite{Massen_2005}.

\noindent{\bf (iii) Extremal optimization:} Extremal optimization is a heuristic search procedure proposed by Boettcher and Percus
\cite{Boettcher}, in order to achieve an accuracy comparable with simulated annealing, but with a substantial gain in computer time. It
is based on the optimization of local variables, expressing the contribution of each unit of the system to the global function being
studied.
This technique was used for modularity optimization by Duch and Arenas \cite{PhysRevE.72.}. Generally, this technique maintains a good
trade-off between accuracy and speed, although it sometimes leads to poor results on large networks with many communities
\cite{Fortunato201075}.

\noindent{\bf (iv) Other optimization strategies:} Agarwal and Kempe \cite{Agarwal72} suggested maximization of modularity within the
framework of mathematical programming. Chen et al. \cite{Chen08} used integer linear programming to transform the initial graph into an
optimal target graph consisting of disjoint cliques, which effectively yields a partition. Berry et al. \cite{Berry} formulated
the problem of graph clustering as a {\em facility location problem}, that attempts to minimize a cost
function based on a local
variation of modularity. Lehmann and Hansen \cite{Lehmann} optimized modularity via {\em mean field annealing} \cite{Peterson:1987}. Genetic
algorithms \cite{Holland:1992} have also been used to optimize modularity.

\subsubsection*{Modifications of Modularity}
In the most recent literature on graph clustering several modifications and extensions of modularity can be found. Modularity can be easily
extended to graphs with weighted edges \cite{PhysRevE.70}, directed graphs \cite{Phys118703}. Kim et al. \cite{KSJ} proposed a
different definition based on diffusion on directed graphs, inspired by Google's PageRank algorithm. Rosvall and Bergstrom raised similar
objections \cite{Rosvall290}. Gaertler et al. \cite{Gaertler:2007} introduced quality measures based on modularity's principle of the
comparison between a variable relative to the original graph and the corresponding variable of a null model. Another generalization of
modularity was recently suggested by
Arenas et al. \cite{Arenas2007Motif}. Expressions of modularity for bipartite graphs were suggested by Guimera et al. \cite{Guimera:2007}
and
Barber \cite{barber2007mac}. However, community detection using modularity has certain issues including resolution limit,
degeneracy of solutions and asymptotic growth \cite{gmc2010}. To address these issues, multi-resolution versions of modularity
\cite{Arenas} were proposed to allow researchers to specify a tunable target resolution limit parameter. He et al.~\cite{Dongxiao}
considered different community densities as good quality measures for community identification, which do not suffer from resolution limits. 
Furthermore, Lancichinetti and Fortunato~\cite{santo_11} stated that even those multi-resolution versions of modularity are not only
inclined to merge the smallest well-formed communities, but also to split the largest well-formed communities; some of these problems were
addressed and partially resolved by Chan et al.~\cite{Chen_2013} recently. 

\subsubsection*{Dynamic Algorithms}
Here we describe methods employing processes running on the graph, focusing on spin-spin interactions, random walks and synchronization.

\noindent{\bf (i) Spin models:} The Potts model is among the most popular models in statistical mechanics \cite{Wu235}. It describes a
system of spins that can be in
different states. Based on this idea, Reichardt and Bornholdt
\cite{reichardt2004} proposed a method to detect communities that maps the graph onto a zero-temperature q-Potts model with nearest-neighbor
interactions. In another work, Son et al. \cite{SonSW} have presented a clustering technique based on the {\em Ferromagnetic Random
Field Ising Model} (FRFIM).

\noindent{\bf (ii) Random walk:}
Random walks \cite{hughes1995} can also be useful to find communities. If a graph has a strong community structure, a random walker spends a
long time inside a community due to the high density of internal edges and consequent number of paths that could be followed. Zhou
\cite{Zhou:2003a} used random walks to define a distance between pairs of vertices: the distance $d_{ij}$ between $i$ and $j$ is the
average number of edges that a random walker has to cross to reach $j$ starting from $i$.  A different distance measure between vertices
based on random walks was introduced by Latapy and Pons \cite{Yolum} where the
distance is
calculated from the probabilities that the random walker moves from a vertex to another in a fixed number of steps. Hu et al.
\cite{Hu026121} designed a graph clustering technique based on a signaling process between vertices, somewhat resembling diffusion. Dongen,
in his PhD thesis, described the {\em Markov Cluster Algorithm} (MCL) \cite{vandongen00}.

\subsubsection*{Statistical Inference based Methods}\label{info}
Statistical inference aims at deducing properties of data sets, starting from a set of observation and model hypotheses.
If the data set is a graph, the model, based on hypotheses on how vertices are connected to each other, has to fit the actual graph.

\noindent{\bf (i) Generative models:}
Most of the methods adopted Bayesian inference \cite{winkler1972}, in which the best fit is obtained through the maximization of a
likelihood (generative models). Hastings \cite{Hastings035102} chose a {\em planted partition model} of network with communities.
Newman and Leicht \cite{Newman05062007} proposed a similar method based on a mixture model and the expectation-maximization
technique. Another technique similar to that by Newman and Leicht was designed by Ren et al. \cite{Ren036111} based on the {\em group
fractions}.
Maximum likelihood estimation was used by \u{C}opi\u{c} et al. \cite{Μcopi05} to define an axiomatization of the problem of graph
clustering and its related concepts.  Hofman and Wiggins \cite{Hofman2007} proposed a general Bayesian approach to the
problem of graph clustering. The main limitation of these methods comes from high memory requirements.

\noindent{\bf (ii) Information theoretic approach:}
The modular structure of a graph can be considered as a compressed description of the graph to approximate the whole information contained
in its adjacency matrix. Rosvall and Bergstrom \cite{Rosvall01052007} envisioned a communication process in which a partition of a graph in
communities represents a synthesis of the full structure that a signaler sends to a receiver, who tries to infer the original graph topology
from it. The same idea is the basis of an earlier method by Sun et al. \cite{Sun:2007}, which was originally designed for bipartite
graphs evolving in time. In a recent paper, Rosvall and Bergstrom \cite{Rosvall290} pursued the same idea of describing a graph by using
less information than that encoded in the full adjacency matrix. The goal is to optimally compress the information needed to describe the
process of information diffusion across the graph. Chakrabarti \cite{Chakrabarti04a} has applied
the minimum description length principle to put the adjacency matrix of a graph into the (approximately) block diagonal form representing
the best trade-off between having a limited number of blocks, for a good compression of the graph topology, and having very homogeneous
blocks, for a compact description of their structure.

\subsubsection*{Other Methods}
Here we describe some algorithms that do not fit in the previous categories. Raghavan et al. \cite{Raghavan} designed a simple and fast
method based on {\em label propagation}. The main advantage of the method is the fact that it does not need any
information on the number and the size of the clusters. It does not need any parameter, either. In a recent paper, Tib{\'e}ly and
Kert{\'e}sz \cite{Tiby20084982} showed that the method is equivalent to finding the local energy minima of a simple zero-temperature kinetic
Potts model.  A recent methodology introduced by Papadopoulos et al. \cite{Skusa}, called {\em
Bridge Bounding}, is similar to the L-shell algorithm, but here the cluster around a vertex grows until one ``hits'' the boundary edges.
Another method, where communities are defined based on a local criterion, was presented by Eckmann and Moses
\cite{Eckmann300}. Long et al. \cite{Long:2007} have devised an interesting technique that is able to detect various types of vertex groups,
not necessarily communities. Zarei and Samani \cite{Zarei200} remarked that there is a symmetry between community structure and
anti-community (multipartite) structure, when one considers a graph and its complement, whose edges are the missing edges of the original
graph. 

\subsection{Overlapping Community Detection}
There has been a class of algorithms for network clustering, which allow nodes belonging to more than one community. As discussed in
\cite{Xie_2013s}, we shall discuss the proposed algorithms by categorizing them into five classes.

\subsubsection*{Clique Percolation Algorithms}
The clique percolation method (CPM) is based on the assumption that a community consists of overlapping sets of fully connected subgraphs
and detects communities by searching for adjacent cliques. {\em CFinder} is the implementation of CPM, whose time complexity is polynomial
in many applications \cite{Palla05}. However, it also fails to terminate in many large social networks. Following this, CPMw
\cite{Farkas180} introduces a subgraph intensity threshold for weighted networks. Only k-cliques with intensity larger than a fixed
threshold are included
into a community. Instead of processing all values of $k$, SCP \cite{Kumpula08} finds clique communities of a given size. Despite their
conceptual simplicity, an usual criticism is that CPM-like algorithms are more like pattern matching rather than
finding communities since
they aim to find specific, localized structure in a network.

\subsubsection*{Link Partitioning Algorithms}
On the other hand, few algorithms trying to  partition links instead of nodes to discover community structure have also been explored. A
node in the original graph is called overlapping if links connected to it are put in more than one cluster. Ahn et al. \cite{nature2010}
proposed a method where links are partitioned via hierarchical clustering of edge similarity.  Evans \cite{Evans0638} projected
the network into a weighted {\em line graph}, whose nodes are the links of the original graph, then applied the node partitioning
algorithm. {\em CDAEO} \cite{5680851} provides a post-processing procedure to determine the extent of overlapping. Kim and Jeong
\cite{Kim0257} extended the map equation method \cite{Rosvall290} to the line graph, which encodes the path
of the random walk on the line network under the Minimum Description Length principle. 

\subsubsection*{Local Expansion and Optimization Algorithms}
Algorithms utilizing local expansion and optimization rely on growing a natural community or a partial community 
\cite{Lancichinetti033015}.  Baumes et al. \cite{BaumesGKMP05} proposed a two-step process: first, nodes are ranked according to some
criterion, then the
process iteratively removes highly ranked nodes until small, disjoint cluster cores are formed. Lancichinetti et al. \cite{Lancichinetti}
proposed an algorithm called {\em LFM} which expands a community from a random seed node to form a natural community until a fitness
function becomes locally maxima. Havemann et al. proposed MONC \cite{Havemann2010} which uses the modified fitness function of LFM that
allows a single node to be considered a community by itself.  Lancichinetti et al. further proposed {\em OSLOM}
\cite{pone.0018961} that  tests the statistical significance of a cluster \cite{Bianconi1407} with respect to a global null model
(i.e., the random graph generated by the configuration model \cite{Molloy:1995} during community expansion).

Chen et al. \cite{Chen20104177} proposed selecting a node with maximal node strength based on two quantities: belonging degree and the
modified
modularity. Cazabet et al. \cite{5591234} proposed {\em iLCD} which is capable of detecting both static and temporal communities. Given a
set of edges created at some time step, iLCD updates the existing communities by adding a new node if its number of second neighbors and
number of robust second neighbors are greater than expected values. 

Seeds are very important for many local optimization algorithm. A clique has been shown to be a better alternative over an individual node
as a seed. Shen et al. \cite{Shen20091706} in their algorithm {\em EAGLE} used the agglomerative framework to produce a dendrogram. Similar
to EAGLE, {\em GCE} \cite{Lee10} identifies maximum cliques as seed communities. 


\subsubsection*{Fuzzy Detection}
Fuzzy community detection algorithms quantify the strength of association between all pairs of nodes and communities.  Nepusz
\cite{nepusz07b} modeled the
overlapping community detection as a nonlinear constrained optimization problem which can be solved by simulated annealing methods. Zhang
et al. \cite{Zhang2007483} proposed an algorithm based on the spectral clustering framework \cite{PhysRevE.74.036104}. There is another
algorithm called {\em FOG}  \cite{5473438} which tries to infer groups based on link evidence. Similar mixture models can also be
constructed as a generative model for nodes \cite{4781180}. In {\em SSDE} \cite{Magdon-IsmailP11}, the network is first mapped into a
$d$-dimensional space using the spectral clustering method. A Gaussian Mixture Model (GMM) is then trained via Expectation-Maximization
algorithm. The number of communities is determined when the increase in log-likelihood of adding a cluster is not significantly higher than
that of adding a cluster to random data which is uniform over the same space.

\if{0}
{\em Stochastic block model} (SBM) \cite{Nowicki55} is another type of generative model for groups in the network. Latouche et al.
\cite{1220.62083} proposed {\em OSBN} where the latent vector is inferred by maximizing the posterior probability conditioned on the present
of edges. {\em MOSES} \cite{McDaid:2010} combines OSBM with the local optimization scheme, in which the fitness function is defined based
on the observed condition distribution. MOSES greedily expands a community from edges. Unlike OSBM, no connection probability parameters are
required as input. 

\fi

{\em Non-negative Matrix Factorization} (NMF) is a feature extraction and dimensionality reduction technique in machine learning that has
been adapted to community detection. Zhang et al. \cite{Zhang046103} replaced the feature vector used in NMF with the diffusion kernel,
which is a function of the Laplacian of the network. Later Zarei et al. \cite{ZareiP11013} showed that the result would be better if the
matrix is defined by the  correlation matrix of the columns of the Laplacian. Recently, Yang and Leskovec \cite{lesco} proposed BIGCLAM
which is also based on
NMF approach.

Ding et al. \cite{5473438} extended the {\em affinity propagation clustering algorithm} \cite{Frey07} for overlapping community detection,
in which
clusters are identified by representative exemplars. First, nodes are mapped as data points in the Euclidean space via the commute time
kernel (a function of the inverse Laplacian). The similarity between nodes is then measured by the cosine distance.


\subsubsection*{Agent-based and Dynamical Algorithms}
The label propagation algorithm \cite{Raghavan} in which nodes with same label form a community, has been extended to overlapping community
detection by allowing a node to have multiple labels. Gregory proposed {\em COPRA} \cite{Gregory1} in which each node updates its belonging
coefficient by averaging the coefficients from all its neighbors at each time step in a synchronous fashion.  Xie et al. \cite{6137400}
developed {\em SLPA} which is a general speaker-listener based information propagation process. A game-theoretic framework is proposed in
Chen et al. \cite{Chen:2010}, in which a community is associated with
a Nash local equilibrium. A process in which particles walk and compete with each other to occupy nodes is presented by Breve et al.
\cite{Breve:2009}. Different from SLPA and COPRA, this algorithm takes a semi-supervised approach. It requires at least one labeled node per
class.

\subsubsection*{Other Methods}
{\em CONGO} \cite{Gregory:2007} extends Girvan and Newman's divisive clustering algorithm \cite{Girvan02} by allowing a node to split into
multiple copies.  Gregory
\cite{Gregory2009} also proposed to perform disjoint detection algorithms on the network produced by splitting the node into multiple copies
using the split betweenness. Zhang et al. \cite{Zhang:2009} proposed an iterative process that reinforces the network topology and
propinquity that is interpreted as the probability of a pair of nodes belonging to the same community. The propinquity between two
vertices is defined as the sum of the number of direct links, number of common neighbors and the number of links within the common
neighborhood. Kov\'{a}cs et al. \cite{Kovacs10} proposed an approach focusing on centrality-based influence functions. 
% Rees and Gallagher
% \cite{5562742} and Mcauley and Leskovec \cite{Mcauley:2014} proposed an algorithm to extract the overlapping communities from the {\em
% egonet}, which is a subgraph including a center node, its neighbors, and the links around them. 



\subsection{Community Scoring Metrics}
Another important aspect of community detection is to evaluate the detected community structure. If we know the actual community structure
of a network, it would be easier to evaluate the detected communities just by comparing them with the actual community structure. However,
most of the time, collecting the actual ground-truth community structure is difficult, and therefore we rely on the structural property of
the community structure. In this section, we first describe such topology-based community evaluation metrics and then briefly mention few
popular validation metrics that are used to compare the detected community with the ground-truth structure.


\subsubsection*{Topology-based Community Evaluation Metric}
Several metrics for evaluating the quality of community structure have been introduced. The most popular and widely accepted is
Modularity~\cite{ng2002} (see Equation \ref{modularity}). Recently, Fortunato and Barthelemy \cite{Barthelemy} presented a {\it
resolution limit} problem of modularity, essence of which is that optimizing modularity will not find communities smaller than a threshold
size, or weight~\cite{Berry_PRE2011}. The threshold depends on the total number (or total weight) of edges in the network and on the degree
of interconnectedness between communities. Moreover, Good et al. \cite{gmc2010} showed another problem of Modularity called {\it degeneracy
of solutions} that this measure admits an exponential number of high-modularity but structurally distinct solutions from a single graph.
They also studied the limiting behavior of maximizing modularity for one model of infinitely modular networks ({\it asymptotic growth}),
showing that it depends strongly both on the size of the network and on the number of modules it contains, i.e., as we add more modules to
the network, the height of the modularity function converges to 1. To address the resolution limit problem, multi-resolution versions of
modularity~\cite{Arenas} were proposed to allow researchers to specify a tunable target resolution limit parameter.
Lambiotte \cite{Renaud} proposed different types of multi-resolution quality functions to tackle resolution limit problem.
Dongxiao et al. \cite{Dongxiao} considered different community densities as good quality measures for community identification, which do not
suffer from resolution limits. 

In the context of overlapping community evaluation, people attempted to redefine modularity for overlapping community structure. Shen et al.
\cite{Shen20091706} introduced $EQ$, an adaptation of Newman's modularity function designed to support overlapping communities. The
equation for $EQ$ strongly resembles the original modularity function as follows:
\begin{equation}
 EQ=\frac{1}{2m}\sum_{c\in C} \sum_{i\in c,j\in c} \frac{1}{O_iO_j} \left[ A_{ij} - \frac{k_ik_j}{2m} \right]
\end{equation}
where $m$ is the number of edges in the graph, $C$ is the set of communities, and $O_v$ is the number of communities to which the node
$v$ belongs. The presence of an edge between two nodes $v$ and $w$ is represented as the value in the corresponding position of the
adjacency matrix $A_{vw}$.

On the other hand, recently L{\'a}z{\'a}r  et al. \cite{Vicsek}  provided a more complex and potentially more accurate evaluation of the
goodness of an overlapping community structure as follows:
\begin{equation}
 Q_{ov}=\frac{1}{K} \sum\limits_{r=1}^{K} \Bigg [  \frac{\sum\limits_{i\in c_r}  \frac{\sum\limits_{j\in c_r, i \neq j} A_{ij} -
\sum\limits_{j \notin c_r} A_{ij}}{di\cdot s_i } }{n_{c_r}} \cdot  \frac{n^e_{c_r}}{	\dbinom{n_{c_r}}{2}}  \Bigg ]
\end{equation}
where $K$ is the number of communities, $n_{c_r}$ is the number of nodes and $n^e_{c_r}$ is the number of edges that the $r$th cluster $c_r$
contains respectively, $d_i$ is the degree of node $i$, $s_i$ denotes the number of clusters where $i$ belongs to and $A$ is the adjacency
matrix. Note that, since the density of clusters containing one single node (when $n_{c_r}$ = 1) is not defined (because $\dbinom{1}{2}$ is
undefined), the modularity value is set to be zero.

 Ahn et al. \cite{nature2010} described two simple measures to quantify the quality of a community
structure. The first one is {\em Community Coverage} which simply counts the fraction of nodes that belong to at least
one community of three or more nodes. A size of three is chosen since it is the smallest nontrivial community. This measure provides an
estimate of how much of the network is analyzed. The second measure is {\em Overlap Coverage} which counts the average number of
memberships in
nontrivial communities (size at leas three) that nodes are given.




\subsubsection*{Ground-truth Based Community Validation Metrics}\label{sec:ground-truth_metric}
Evaluating the quality of a detected partitioning or cover is nontrivial, and extending evaluation measures from disjoint to overlapping
communities is rarely straightforward. In this section, we discuss some of the popular evaluation metrics which
are often used to compare the detected partition with the ground-truth communities.




\noindent{\bf (i) Purity (PU):} The Purity measure \cite{nmi} is historically the first one used in the context of community
detection. Let us assume that $X=\{x_1,x_2,...,x_I\}$ and $Y=\{y_1,y_2,...,y_J\}$ be the two partitions of the same set. To denote the
cardinalities, we use $n$ for the total number of elements in the partitioned set, and $n_{ij}=|x_i \cap y_j|$  for the intersection of two
parts. We also note $n_{i+}=|x_i|$ and $n_{+j}=|y_j|$ the part size. The purity of a part $x_i$ relative to the other partition $Y$ is
expressed as $PU(x_i,Y)=\underset{j} {\mathrm{max}} \frac{n_{ij}}{n_{i+}}$. The total purity of partition $X$ relative to partition $Y$ is
obtained as follows: $ PU(X,Y)=\sum_i \frac{n_{i+}}{n} PU(x_i,Y)$.

It is important to notice the purity is not a symmetric measure. Therefore, the usual approach is to take the harmonic mean of
$PU(X,Y)$ and $PU(Y,X)$.
The upper bound is 1, it corresponds to a perfect match between the partitions. The lower bound is 0 and indicates the opposite. 


\noindent{\bf (ii) Rand Index (RI):} The Rand Index \cite{rand1971} is a way of comparing disjoint clustering solutions that is based on
pairs of the objects being clustered. Two solutions are said to agree on a pair of objects if they each put both objects into the same
cluster or each into different clusters. The Rand Index can then be formalized as follows:
\begin{equation}
 RI=\frac{(a+d)}{N}
\end{equation}
where $N$ is the number of pairs of objects, $a$ is the number of times the solutions agree on putting a pair in the same cluster and $d$ is
the number of times the solutions agree on putting a pair in different clusters. That is, the Rand Index is the number of pairs that are
agreed on by the two solutions divided by the total number of pairs.

An improvement to the Rand Index is the {\em Adjusted Rand Index} ($ARI$) \cite{hubert1985} which adjusts the level of agreement according
to the expected amount of agreement based on chance. 

\noindent{\bf (iii) Omega Index:} The Omega Index \cite{collins1988} builds on both the Rand Index and Adjusted Rand Index by accounting for
disjoint solutions and correcting for chance agreement. The Omega Index considers the number of clusters in which a pair of objects is
together. The observed agreement between two partitions $S1$ and $S2$ is calculated by: $Obs(S1,S2)=\sum_{j=0}^{min(J,K)} A_j/N$, where $J$
and $K$ represent
the maximum number of clusters in which any pair of objects appears together in partitions 1 and 2, respectively, $A_j$ is the number of the
pairs agreed by both partitions to be assigned to number of clusters $j$, and $N$ is again the number of pairs of objects. The expected
agreement is given by: $Exp(S1,S2)=\sum_{j=0}^{min(J,K)} N_{j1}N_{j2}/N^2$, where $N_{j1}$ is the total number of pairs assigned to number
of clusters $j$ in partition 1, and $N_{j2}$ is the total number of pairs assigned to number of clusters $j$ in partition 2. The Omega Index
is then calculated as
\begin{equation}
 Omega(S1,S2)=\frac{Obs(S1,S2) - Exp(S1,S2)}{1-Exp(S1,S2)}
\end{equation}
The highest possible score of 1 indicates that two solutions perfectly agree on how each pair of objects is clustered.






\noindent{\bf (iv) Normalized Mutual Information (NMI):} The problem of comparing different community structures can be overcome by
computing
the
Normalized Mutual Information (NMI)~\cite{Vinh:2009}. Let $C$ be the confusion matrix. Also let $N_{ij}$ (elements of the confusion matrix
$C$) be the number of nodes in the intersection of the original community $i$ and the generated community $j$. If $C_A$ denotes the number
of the communities in the ground truth, $C_B$ the number of the generated communities by an arbitrary approach, $N_i$ the sum of row
$i$, $N_j$ the
sum of column $j$, and $N$ the sum of all elements in $C$, then the NMI score between the ground truth partition $A$, and the generated
partition $B$ can be computed as shown in the following equation.
\begin{equation}
 NMI(A,B)=\frac{-2\sum\limits_{i=1}^{C_A}\sum\limits_{j=1}^{C_B}N_{ij}log\frac{N_{ij}N}{N_iN_j}}
{\sum\limits_{i=1}^{C_A}{N_i}log\frac{N_i}{N}+\sum\limits_{j=1}^{C_B}{N_j}log\frac{N_j}{N} }
\end{equation}
The values of NMI range between 0 and 1 where 0 refers to no match with the ground truth and 1 refers to a perfect match. Recently, McDaid
et al. \cite{McDaid} also provided a modified version of NMI, called {\em ONMI} for evaluating  overlapping community structures. 

However, Labatut \cite{Labatut} argued that these measures are not completely relevant in the context of network analysis, because they
ignore the network connectivity.  He proposed the modified versions of these measures where misplacing a high degree vertices would incur
higher penalty compared to a low degree vertices. The modified versions of NMI, ARI and Purity are the weighted versions,
namely Weighted-NMI (W-NMI), Weighted-ARI (W-ARI) and Weighted-Purity (W-PU).  

